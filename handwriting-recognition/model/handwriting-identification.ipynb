{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:08:46.509769Z",
     "iopub.status.busy": "2024-12-04T20:08:46.509035Z",
     "iopub.status.idle": "2024-12-04T20:08:46.514356Z",
     "shell.execute_reply": "2024-12-04T20:08:46.513403Z",
     "shell.execute_reply.started": "2024-12-04T20:08:46.509737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:08:51.09384Z",
     "iopub.status.busy": "2024-12-04T20:08:51.093453Z",
     "iopub.status.idle": "2024-12-04T20:08:51.097931Z",
     "shell.execute_reply": "2024-12-04T20:08:51.097051Z",
     "shell.execute_reply.started": "2024-12-04T20:08:51.093811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Define Dataset and DataLoader\n",
    "data_dir = 'C:\\\\Users\\\\Giuseppe\\\\Documents\\\\tesi\\\\datasets\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:08:54.539245Z",
     "iopub.status.busy": "2024-12-04T20:08:54.538422Z",
     "iopub.status.idle": "2024-12-04T20:08:54.543367Z",
     "shell.execute_reply": "2024-12-04T20:08:54.542552Z",
     "shell.execute_reply.started": "2024-12-04T20:08:54.539214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Transformations: Resize, Normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to fixed size\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:08:59.339403Z",
     "iopub.status.busy": "2024-12-04T20:08:59.338604Z",
     "iopub.status.idle": "2024-12-04T20:09:06.574364Z",
     "shell.execute_reply": "2024-12-04T20:09:06.573456Z",
     "shell.execute_reply.started": "2024-12-04T20:08:59.33937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:09:10.658811Z",
     "iopub.status.busy": "2024-12-04T20:09:10.658438Z",
     "iopub.status.idle": "2024-12-04T20:09:10.706997Z",
     "shell.execute_reply": "2024-12-04T20:09:10.706348Z",
     "shell.execute_reply.started": "2024-12-04T20:09:10.658782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split Dataset: Train/Validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:09:18.304297Z",
     "iopub.status.busy": "2024-12-04T20:09:18.30352Z",
     "iopub.status.idle": "2024-12-04T20:09:18.308182Z",
     "shell.execute_reply": "2024-12-04T20:09:18.307344Z",
     "shell.execute_reply.started": "2024-12-04T20:09:18.304265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:10:49.744922Z",
     "iopub.status.busy": "2024-12-04T20:10:49.744021Z",
     "iopub.status.idle": "2024-12-04T20:10:49.749554Z",
     "shell.execute_reply": "2024-12-04T20:10:49.748839Z",
     "shell.execute_reply.started": "2024-12-04T20:10:49.744885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)  # Adjust output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:11:03.81986Z",
     "iopub.status.busy": "2024-12-04T20:11:03.8191Z",
     "iopub.status.idle": "2024-12-04T20:11:03.823719Z",
     "shell.execute_reply": "2024-12-04T20:11:03.822668Z",
     "shell.execute_reply.started": "2024-12-04T20:11:03.819823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Number of classes (writers)\n",
    "num_classes = len(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:11:23.899627Z",
     "iopub.status.busy": "2024-12-04T20:11:23.898911Z",
     "iopub.status.idle": "2024-12-04T20:11:24.930132Z",
     "shell.execute_reply": "2024-12-04T20:11:24.929371Z",
     "shell.execute_reply.started": "2024-12-04T20:11:23.899596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Giuseppe\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Giuseppe\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Giuseppe/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:02<00:00, 21.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCNN(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:14:10.060327Z",
     "iopub.status.busy": "2024-12-04T20:14:10.059666Z",
     "iopub.status.idle": "2024-12-04T20:14:10.064758Z",
     "shell.execute_reply": "2024-12-04T20:14:10.063887Z",
     "shell.execute_reply.started": "2024-12-04T20:14:10.060293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:14:16.981234Z",
     "iopub.status.busy": "2024-12-04T20:14:16.980902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 6.7377, Train Accuracy: 3.90%, Validation Accuracy: 1.62%\n",
      "Epoch [2/25], Loss: 5.4862, Train Accuracy: 7.31%, Validation Accuracy: 7.47%\n",
      "Epoch [3/25], Loss: 4.4971, Train Accuracy: 16.08%, Validation Accuracy: 11.69%\n",
      "Epoch [4/25], Loss: 3.5495, Train Accuracy: 28.35%, Validation Accuracy: 21.10%\n",
      "Epoch [5/25], Loss: 2.6105, Train Accuracy: 42.89%, Validation Accuracy: 28.90%\n",
      "Epoch [6/25], Loss: 1.9114, Train Accuracy: 53.37%, Validation Accuracy: 23.05%\n",
      "Epoch [7/25], Loss: 1.2812, Train Accuracy: 68.81%, Validation Accuracy: 44.81%\n",
      "Epoch [8/25], Loss: 0.8216, Train Accuracy: 80.99%, Validation Accuracy: 41.56%\n",
      "Epoch [9/25], Loss: 0.5466, Train Accuracy: 88.38%, Validation Accuracy: 37.01%\n",
      "Epoch [10/25], Loss: 0.3895, Train Accuracy: 92.45%, Validation Accuracy: 42.53%\n",
      "Epoch [11/25], Loss: 0.2391, Train Accuracy: 95.21%, Validation Accuracy: 35.06%\n",
      "Epoch [12/25], Loss: 0.1826, Train Accuracy: 96.99%, Validation Accuracy: 13.31%\n",
      "Epoch [13/25], Loss: 0.1203, Train Accuracy: 98.38%, Validation Accuracy: 27.27%\n",
      "Epoch [14/25], Loss: 0.0744, Train Accuracy: 98.86%, Validation Accuracy: 19.81%\n",
      "Epoch [15/25], Loss: 0.0540, Train Accuracy: 99.68%, Validation Accuracy: 54.55%\n",
      "Epoch [16/25], Loss: 0.0249, Train Accuracy: 100.00%, Validation Accuracy: 56.82%\n",
      "Epoch [17/25], Loss: 0.0368, Train Accuracy: 99.68%, Validation Accuracy: 58.44%\n",
      "Epoch [18/25], Loss: 0.0392, Train Accuracy: 99.43%, Validation Accuracy: 48.70%\n",
      "Epoch [19/25], Loss: 0.0403, Train Accuracy: 99.43%, Validation Accuracy: 54.22%\n",
      "Epoch [20/25], Loss: 0.0502, Train Accuracy: 99.35%, Validation Accuracy: 45.45%\n",
      "Epoch [21/25], Loss: 0.0885, Train Accuracy: 98.70%, Validation Accuracy: 37.66%\n",
      "Epoch [22/25], Loss: 0.2379, Train Accuracy: 94.48%, Validation Accuracy: 36.36%\n",
      "Epoch [23/25], Loss: 0.3110, Train Accuracy: 92.53%, Validation Accuracy: 12.99%\n",
      "Epoch [24/25], Loss: 0.3615, Train Accuracy: 91.06%, Validation Accuracy: 14.61%\n",
      "Epoch [25/25], Loss: 0.2155, Train Accuracy: 94.80%, Validation Accuracy: 27.60%\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the Model\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracies.append(100 * correct / total)\n",
    "\n",
    "    # Validation Accuracy\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "    val_accuracies.append(100 * correct_val / total_val)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "          f\"Train Accuracy: {train_accuracies[-1]:.2f}%, Validation Accuracy: {val_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'handwriting_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 5: Plot Training and Validation Metrics\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split Dataset: Train/Test\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train, test = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_load = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_load = DataLoader(test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_author(new_author_data_path, model, num_classes, train_loader, val_loader, epochs=10):\n",
    "    \"\"\"\n",
    "    Function to add a new author to the handwriting identification system.\n",
    "\n",
    "    Args:\n",
    "        new_author_data_path (str): Path to the new author's data.\n",
    "        model (torch.nn.Module): Pre-trained model.\n",
    "        num_classes (int): Current number of classes in the model.\n",
    "        train_loader (DataLoader): DataLoader for existing training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        epochs (int): Number of fine-tuning epochs.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Load new author's data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    new_author_dataset = ImageFolder(root=new_author_data_path, transform=transform)\n",
    "    new_author_loader = DataLoader(new_author_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Update the final layer of the model to include the new class\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes + 1)  # Add one more class\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Fine-tuning\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Train on both new author and existing data\n",
    "        for images, labels in new_author_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(new_author_loader):.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to new author's data\n",
    "new_author_data_path = \"/path/to/new/author/data\"\n",
    "\n",
    "# Add new author to the system\n",
    "model = add_new_author(\n",
    "    new_author_data_path=new_author_data_path,\n",
    "    model=model,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Salva il modello aggiornato\n",
    "torch.save(model.state_dict(), 'handwriting_model_updated.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize correct predictions\n",
    "model.eval()\n",
    "correct_images, correct_labels, predicted_labels = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_load:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        mask = (predicted == labels)\n",
    "        correct_images.extend(images[mask].cpu())\n",
    "        correct_labels.extend(labels[mask].cpu())\n",
    "        predicted_labels.extend(predicted[mask].cpu())\n",
    "        if len(correct_images) >= 10:  # Display 10 correct images\n",
    "            break\n",
    "\n",
    "# Visualize correct predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    img = correct_images[i].permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n",
    "    img = img * 0.5 + 0.5  # Denormalize\n",
    "    plt.imshow(img.numpy())\n",
    "    plt.title(f\"Label: {correct_labels[i].item()}\\nPred: {predicted_labels[i].item()}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1094263,
     "sourceId": 1840441,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "bio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
