{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T20:08:46.509769Z",
     "iopub.status.busy": "2024-12-04T20:08:46.509035Z",
     "iopub.status.idle": "2024-12-04T20:08:46.514356Z",
     "shell.execute_reply": "2024-12-04T20:08:46.513403Z",
     "shell.execute_reply.started": "2024-12-04T20:08:46.509737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import finetuning_cnn\n",
    "import importlib\n",
    "importlib.reload(finetuning_cnn)\n",
    "from finetuning_cnn import FineTuningCNN\n",
    "\n",
    "import resnet_arcface\n",
    "importlib.reload(resnet_arcface)\n",
    "from resnet_arcface import ArcFaceNet\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomRotation(5),           # Piccole rotazioni Â±5Â°\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),  # Traslazioni leggere\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),        # Leggera variazione luminositÃ /contrasto\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Transformations: Resize, Normalize\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to fixed size\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero train samples: 550\n",
      "Numero test samples: 150\n",
      "Numero classi (label ricodificati): 100\n",
      "Numero di classi (utenti autorizzati): 100\n"
     ]
    }
   ],
   "source": [
    "# --- Dataset base (senza transform) ---\n",
    "dataset = ImageFolder(root='../../IAM+RIMES')  # nessuna transform qui\n",
    "\n",
    "# --- Carica lo split ---\n",
    "#split = torch.load('splits/dataset_split.pth')\n",
    "#train_indices = split['train_indices']\n",
    "#val_indices = split['val_indices']\n",
    "split = torch.load('splits/IAM+RIMES.pth')\n",
    "\n",
    "train_indices = split['train_indices']\n",
    "test_indices = split['test_indices']\n",
    "label_map = split['label_map']\n",
    "\n",
    "print(f\"Numero train samples: {len(train_indices)}\")\n",
    "print(f\"Numero test samples: {len(test_indices)}\")\n",
    "print(f\"Numero classi (label ricodificati): {len(label_map)}\")\n",
    "\n",
    "# --- Applica lo split ---\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, test_indices)\n",
    "\n",
    "class TransformedSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform, label_map):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map  # dizionario {label_originale: label_ricodificato}\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.subset[idx]\n",
    "        if label in self.label_map:\n",
    "            mapped_label = self.label_map[label]\n",
    "        else:\n",
    "            # Label non autorizzata, assegna -1 o altra label \"speciale\"\n",
    "            mapped_label = -1\n",
    "        return self.transform(img), mapped_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "# --- Applica le trasformazioni specifiche ---\n",
    "train_data = TransformedSubset(train_subset, transform_train, label_map)\n",
    "\n",
    "# --- Calcola il numero di classi a partire dal training set ---\n",
    "all_labels = [label for _, label in train_subset]\n",
    "\n",
    "num_classes = len(label_map)\n",
    "print(f\"Numero di classi (utenti autorizzati): {num_classes}\")\n",
    "\n",
    "val_data = TransformedSubset(val_subset, transform_test, label_map)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "0 656\n"
     ]
    }
   ],
   "source": [
    "all_labels = [label for _, label in train_subset]\n",
    "num_classes = len(set(all_labels))\n",
    "print(num_classes)\n",
    "print(min(all_labels), max(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Inizio warm-up...\n",
      "[Warm-up Epoch 1/5] Loss: 6.3031, Train Acc: 1.27%, Val Overall Acc: 33.33%, FAR: 0.00%, FRR: 100.00%, Avg Conf: 0.10\n",
      "[Warm-up Epoch 2/5] Loss: 5.3672, Train Acc: 2.36%, Val Overall Acc: 33.33%, FAR: 0.00%, FRR: 100.00%, Avg Conf: 0.07\n",
      "[Warm-up Epoch 3/5] Loss: 12.8814, Train Acc: 0.00%, Val Overall Acc: 33.33%, FAR: 0.00%, FRR: 100.00%, Avg Conf: 0.07\n",
      "[Warm-up Epoch 4/5] Loss: 12.6907, Train Acc: 0.00%, Val Overall Acc: 33.33%, FAR: 0.00%, FRR: 100.00%, Avg Conf: 0.09\n",
      "[Warm-up Epoch 5/5] Loss: 12.5400, Train Acc: 0.00%, Val Overall Acc: 33.33%, FAR: 0.00%, FRR: 100.00%, Avg Conf: 0.13\n",
      "ðŸ”§ Inizio fine-tuning...\n",
      "[Fine-tune Epoch 1/30] Loss: 11.8936, Train Acc: 0.00%, Val Overall Acc: 34.67%, FAR: 0.00%, FRR: 98.00%, Avg Conf: 0.16\n",
      "[Fine-tune Epoch 2/30] Loss: 11.0737, Train Acc: 0.00%, Val Overall Acc: 32.00%, FAR: 14.00%, FRR: 95.00%, Avg Conf: 0.26\n",
      "[Fine-tune Epoch 3/30] Loss: 10.3729, Train Acc: 0.00%, Val Overall Acc: 34.67%, FAR: 14.00%, FRR: 91.00%, Avg Conf: 0.34\n",
      "[Fine-tune Epoch 4/30] Loss: 9.7004, Train Acc: 0.91%, Val Overall Acc: 32.00%, FAR: 40.00%, FRR: 82.00%, Avg Conf: 0.45\n",
      "[Fine-tune Epoch 5/30] Loss: 8.9400, Train Acc: 4.18%, Val Overall Acc: 32.67%, FAR: 58.00%, FRR: 72.00%, Avg Conf: 0.59\n",
      "[Fine-tune Epoch 6/30] Loss: 8.2845, Train Acc: 8.18%, Val Overall Acc: 34.00%, FAR: 64.00%, FRR: 67.00%, Avg Conf: 0.66\n",
      "[Fine-tune Epoch 7/30] Loss: 7.3558, Train Acc: 13.64%, Val Overall Acc: 35.33%, FAR: 84.00%, FRR: 55.00%, Avg Conf: 0.78\n",
      "[Fine-tune Epoch 8/30] Loss: 6.4647, Train Acc: 20.00%, Val Overall Acc: 42.67%, FAR: 84.00%, FRR: 44.00%, Avg Conf: 0.79\n",
      "[Fine-tune Epoch 9/30] Loss: 5.3523, Train Acc: 31.82%, Val Overall Acc: 48.67%, FAR: 80.00%, FRR: 37.00%, Avg Conf: 0.81\n",
      "[Fine-tune Epoch 10/30] Loss: 4.8672, Train Acc: 38.91%, Val Overall Acc: 48.67%, FAR: 84.00%, FRR: 35.00%, Avg Conf: 0.87\n",
      "[Fine-tune Epoch 11/30] Loss: 4.6378, Train Acc: 43.45%, Val Overall Acc: 44.00%, FAR: 80.00%, FRR: 44.00%, Avg Conf: 0.84\n",
      "[Fine-tune Epoch 12/30] Loss: 4.1358, Train Acc: 43.64%, Val Overall Acc: 48.00%, FAR: 86.00%, FRR: 35.00%, Avg Conf: 0.86\n",
      "[Fine-tune Epoch 13/30] Loss: 3.4803, Train Acc: 56.55%, Val Overall Acc: 50.67%, FAR: 88.00%, FRR: 30.00%, Avg Conf: 0.86\n",
      "[Fine-tune Epoch 14/30] Loss: 3.4866, Train Acc: 56.00%, Val Overall Acc: 54.00%, FAR: 78.00%, FRR: 30.00%, Avg Conf: 0.86\n",
      "[Fine-tune Epoch 15/30] Loss: 3.3380, Train Acc: 58.73%, Val Overall Acc: 54.67%, FAR: 80.00%, FRR: 28.00%, Avg Conf: 0.87\n",
      "[Fine-tune Epoch 16/30] Loss: 3.0261, Train Acc: 62.36%, Val Overall Acc: 55.33%, FAR: 96.00%, FRR: 19.00%, Avg Conf: 0.92\n",
      "[Fine-tune Epoch 17/30] Loss: 2.7033, Train Acc: 68.00%, Val Overall Acc: 57.33%, FAR: 86.00%, FRR: 21.00%, Avg Conf: 0.90\n",
      "[Fine-tune Epoch 18/30] Loss: 2.6882, Train Acc: 70.36%, Val Overall Acc: 58.00%, FAR: 82.00%, FRR: 22.00%, Avg Conf: 0.89\n",
      "[Fine-tune Epoch 19/30] Loss: 2.6297, Train Acc: 72.55%, Val Overall Acc: 57.33%, FAR: 88.00%, FRR: 20.00%, Avg Conf: 0.91\n",
      "[Fine-tune Epoch 20/30] Loss: 2.3417, Train Acc: 72.55%, Val Overall Acc: 60.67%, FAR: 80.00%, FRR: 19.00%, Avg Conf: 0.90\n",
      "[Fine-tune Epoch 21/30] Loss: 2.0896, Train Acc: 76.36%, Val Overall Acc: 60.00%, FAR: 88.00%, FRR: 16.00%, Avg Conf: 0.90\n",
      "[Fine-tune Epoch 22/30] Loss: 2.3622, Train Acc: 77.64%, Val Overall Acc: 61.33%, FAR: 90.00%, FRR: 13.00%, Avg Conf: 0.92\n",
      "[Fine-tune Epoch 23/30] Loss: 2.3056, Train Acc: 77.45%, Val Overall Acc: 60.67%, FAR: 88.00%, FRR: 15.00%, Avg Conf: 0.92\n",
      "[Fine-tune Epoch 24/30] Loss: 2.1983, Train Acc: 81.82%, Val Overall Acc: 60.67%, FAR: 88.00%, FRR: 15.00%, Avg Conf: 0.92\n",
      "[Fine-tune Epoch 25/30] Loss: 2.2515, Train Acc: 80.91%, Val Overall Acc: 56.67%, FAR: 94.00%, FRR: 18.00%, Avg Conf: 0.92\n",
      "[Fine-tune Epoch 26/30] Loss: 2.0315, Train Acc: 81.82%, Val Overall Acc: 59.33%, FAR: 88.00%, FRR: 17.00%, Avg Conf: 0.92\n",
      "[Fine-tune Epoch 27/30] Loss: 2.0977, Train Acc: 79.82%, Val Overall Acc: 61.33%, FAR: 90.00%, FRR: 13.00%, Avg Conf: 0.92\n",
      "[Fine-tune Epoch 28/30] Loss: 2.2384, Train Acc: 82.73%, Val Overall Acc: 60.00%, FAR: 88.00%, FRR: 16.00%, Avg Conf: 0.92\n",
      "[Fine-tune Epoch 29/30] Loss: 2.2939, Train Acc: 79.09%, Val Overall Acc: 63.33%, FAR: 84.00%, FRR: 13.00%, Avg Conf: 0.92\n",
      "[Fine-tune Epoch 30/30] Loss: 1.9925, Train Acc: 81.27%, Val Overall Acc: 62.67%, FAR: 88.00%, FRR: 12.00%, Avg Conf: 0.92\n"
     ]
    }
   ],
   "source": [
    "def get_optimizers(model, stage=\"head\"):\n",
    "    if stage == \"head\":\n",
    "        for param in model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {\"params\": model.embedding_layer.parameters(), \"lr\": 1e-3},\n",
    "            {\"params\": model.arcface.parameters(), \"lr\": 1e-3}\n",
    "        ], weight_decay=5e-4)\n",
    "    elif stage == \"finetune\":\n",
    "        for param in model.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {\"params\": model.backbone.parameters(), \"lr\": 1e-4},\n",
    "            {\"params\": model.embedding_layer.parameters(), \"lr\": 1e-3},\n",
    "            {\"params\": model.arcface.parameters(), \"lr\": 1e-3}\n",
    "        ], weight_decay=5e-4)\n",
    "    return optimizer\n",
    "\n",
    "# -----------------------------\n",
    "# Setup modello e parametri\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ArcFaceNet(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "threshold = 0.6\n",
    "\n",
    "# Esempio di calcolo probabilitÃ  stabile:\n",
    "# logits, embeddings = model(images, labels)\n",
    "# probs = F.softmax(logits, dim=1)  # giÃ  numericamente stabile grazie alla sottrazione del max\n",
    "\n",
    "\n",
    "num_epochs_warmup = 5\n",
    "lr_warmup = 5e-3  # leggermente piÃ¹ alto\n",
    "num_epochs_finetune = 30\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_auth_accuracies = []\n",
    "val_far_rates = []\n",
    "val_frr_rates = []\n",
    "val_overall_accuracies = []\n",
    "\n",
    "best_overall_acc = 0.0  # per salvare il checkpoint migliore\n",
    "\n",
    "# -----------------------------\n",
    "# Warm-up epochs\n",
    "# -----------------------------\n",
    "optimizer = get_optimizers(model, stage=\"head\")\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr_warmup\n",
    "\n",
    "print(\"Inizio warm-up...\")\n",
    "for epoch in range(num_epochs_warmup):\n",
    "    # --- TRAIN ---\n",
    "    model.train()\n",
    "    running_loss = correct_train = total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # prime 2 epoche: forward open-set\n",
    "        logits, embeddings = model(images, labels=None if epoch < 2 else labels)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracies.append(100 * correct_train / total_train)\n",
    "\n",
    "    # --- VALIDATION OPEN-SET ---\n",
    "    model.eval()\n",
    "    correct_auth = total_auth = 0\n",
    "    false_accepts = false_rejects = 0\n",
    "    total_unauth = total_test = correct_total = 0\n",
    "    confidence_sum = confidence_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits, embeddings = model(images, labels=None)\n",
    "            # softmax stabile\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            max_probs, predicted = torch.max(probs, 1)\n",
    "\n",
    "            for pred, label, conf in zip(predicted, labels, max_probs):\n",
    "                label, pred, conf = label.item(), pred.item(), conf.item()\n",
    "                confidence_sum += conf\n",
    "                confidence_count += 1\n",
    "\n",
    "                if conf < threshold:\n",
    "                    pred = -1\n",
    "\n",
    "                if label == -1:\n",
    "                    total_unauth += 1\n",
    "                    if pred != -1:\n",
    "                        false_accepts += 1\n",
    "                else:\n",
    "                    total_auth += 1\n",
    "                    if pred == label:\n",
    "                        correct_auth += 1\n",
    "                    else:\n",
    "                        false_rejects += 1\n",
    "\n",
    "                if (label != -1 and pred == label) or (label == -1 and pred == -1):\n",
    "                    correct_total += 1\n",
    "                total_test += 1\n",
    "\n",
    "    auth_acc = 100 * correct_auth / total_auth if total_auth > 0 else 0\n",
    "    far = 100 * false_accepts / total_unauth if total_unauth > 0 else 0\n",
    "    frr = 100 * false_rejects / total_auth if total_auth > 0 else 0\n",
    "    overall_acc = 100 * correct_total / total_test if total_test > 0 else 0\n",
    "    avg_confidence = confidence_sum / confidence_count if confidence_count > 0 else 0.0\n",
    "\n",
    "    val_auth_accuracies.append(auth_acc)\n",
    "    val_far_rates.append(far)\n",
    "    val_frr_rates.append(frr)\n",
    "    val_overall_accuracies.append(overall_acc)\n",
    "\n",
    "    print(f\"[Warm-up Epoch {epoch+1}/{num_epochs_warmup}] \"\n",
    "          f\"Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "          f\"Val Overall Acc: {overall_acc:.2f}%, FAR: {far:.2f}%, FRR: {frr:.2f}%, Avg Conf: {avg_confidence:.2f}\")\n",
    "\n",
    "    # salva checkpoint migliore\n",
    "    if overall_acc > best_overall_acc:\n",
    "        best_overall_acc = overall_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Fine-tune completo\n",
    "# -----------------------------\n",
    "optimizer = get_optimizers(model, stage=\"finetune\")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs_finetune)\n",
    "print(\"Inizio fine-tuning...\")\n",
    "\n",
    "for epoch in range(num_epochs_finetune):\n",
    "    model.train()\n",
    "    running_loss = correct_train = total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits, embeddings = model(images, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracies.append(100 * correct_train / total_train)\n",
    "\n",
    "    # --- VALIDATION OPEN-SET ---\n",
    "    model.eval()\n",
    "    correct_auth = total_auth = 0\n",
    "    false_accepts = false_rejects = 0\n",
    "    total_unauth = total_test = correct_total = 0\n",
    "    confidence_sum = confidence_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits, embeddings = model(images, labels=None)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            max_probs, predicted = torch.max(probs, 1)\n",
    "\n",
    "            for pred, label, conf in zip(predicted, labels, max_probs):\n",
    "                label, pred, conf = label.item(), pred.item(), conf.item()\n",
    "                confidence_sum += conf\n",
    "                confidence_count += 1\n",
    "\n",
    "                if conf < threshold:\n",
    "                    pred = -1\n",
    "\n",
    "                if label == -1:\n",
    "                    total_unauth += 1\n",
    "                    if pred != -1:\n",
    "                        false_accepts += 1\n",
    "                else:\n",
    "                    total_auth += 1\n",
    "                    if pred == label:\n",
    "                        correct_auth += 1\n",
    "                    else:\n",
    "                        false_rejects += 1\n",
    "\n",
    "                if (label != -1 and pred == label) or (label == -1 and pred == -1):\n",
    "                    correct_total += 1\n",
    "                total_test += 1\n",
    "\n",
    "    auth_acc = 100 * correct_auth / total_auth if total_auth > 0 else 0\n",
    "    far = 100 * false_accepts / total_unauth if total_unauth > 0 else 0\n",
    "    frr = 100 * false_rejects / total_auth if total_auth > 0 else 0\n",
    "    overall_acc = 100 * correct_total / total_test if total_test > 0 else 0\n",
    "    avg_confidence = confidence_sum / confidence_count if confidence_count > 0 else 0.0\n",
    "\n",
    "    val_auth_accuracies.append(auth_acc)\n",
    "    val_far_rates.append(far)\n",
    "    val_frr_rates.append(frr)\n",
    "    val_overall_accuracies.append(overall_acc)\n",
    "\n",
    "    print(f\"[Fine-tune Epoch {epoch+1}/{num_epochs_finetune}] \"\n",
    "          f\"Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "          f\"Val Overall Acc: {overall_acc:.2f}%, FAR: {far:.2f}%, FRR: {frr:.2f}%, Avg Conf: {avg_confidence:.2f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if overall_acc > best_overall_acc:\n",
    "        best_overall_acc = overall_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1094263,
     "sourceId": 1840441,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "bio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
