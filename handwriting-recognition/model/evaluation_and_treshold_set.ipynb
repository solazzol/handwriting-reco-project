{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from simple_cnn import SimpleCNN\n",
    "import resnet_arcface\n",
    "importlib.reload(resnet_arcface)\n",
    "from resnet_arcface import ArcFaceNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations: Resize, Normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to fixed size\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomRotation(5),           # Piccole rotazioni ±5°\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),  # Traslazioni leggere\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),        # Leggera variazione luminosità/contrasto\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset base (senza transform) ---\n",
    "dataset = ImageFolder(root='../../../IAM+RIMES')  # nessuna transform qui\n",
    "\n",
    "# --- Carica lo split ---\n",
    "split = torch.load('splits/IAM+RIMES.pth')\n",
    "train_indices = split['train_indices']\n",
    "test_indices = split['test_indices']\n",
    "label_map = split['label_map']\n",
    "\n",
    "# --- Applica lo split ---\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, test_indices)\n",
    "\n",
    "class TransformedSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform, label_map):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map  # dizionario {label_originale: label_ricodificato}\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.subset[idx]\n",
    "        if label in self.label_map:\n",
    "            mapped_label = self.label_map[label]\n",
    "        else:\n",
    "            # Label non autorizzata, assegna -1 o altra label \"speciale\"\n",
    "            mapped_label = -1\n",
    "        return self.transform(img), mapped_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "# --- Applica le trasformazioni specifiche ---\n",
    "train_data = TransformedSubset(train_subset, transform_train, label_map)\n",
    "\n",
    "# --- Calcola il numero di classi a partire dal training set ---\n",
    "all_labels = [label for _, label in train_subset]\n",
    "\n",
    "num_classes = len(label_map)\n",
    "print(f\"Numero di classi (utenti autorizzati): {num_classes}\")\n",
    "\n",
    "val_data = TransformedSubset(val_subset, transform, label_map)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento del modello\n",
    "model_path = 'models/arcface_model.pth'\n",
    "model_id_number = 2\n",
    "#model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "#model.eval()\n",
    "\n",
    "# Load Dataset\n",
    "data_dir = \"../../../IAM+RIMES\"\n",
    "dataset = ImageFolder(root=data_dir)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_id_number == 1:\n",
    "    model = SimpleCNN(num_classes=num_classes)\n",
    "elif model_id_number == 2:\n",
    "    #fourth architecture model\n",
    "    model = ArcFaceNet(num_classes=num_classes).to(device)\n",
    "else:\n",
    "    print(\"Invalid model_id_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate baseline model (before than ArcFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    auc, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_base_model(model, val_loader, threshold=0.5, device='cpu', print_summary=True, plot_roc=True):\n",
    "    model.eval()\n",
    "    total_auth = total_unauth = 0\n",
    "    correct_auth = correct_total = 0\n",
    "    false_accepts = false_rejects = 0\n",
    "\n",
    "    confidences_correct = []\n",
    "    confidences_incorrect = []\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_confs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            max_probs, predicted = torch.max(probs, dim=1)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i].item()\n",
    "                conf = max_probs[i].item()\n",
    "                pred = predicted[i].item()\n",
    "\n",
    "                # Applichiamo la soglia\n",
    "                final_pred = pred if conf >= threshold else -1\n",
    "\n",
    "                all_preds.append(final_pred)\n",
    "                all_labels.append(label)\n",
    "                all_confs.append(conf)\n",
    "\n",
    "                # Metriche principali\n",
    "                if label == -1:  # Non autorizzato\n",
    "                    total_unauth += 1\n",
    "                    if final_pred != -1:\n",
    "                        false_accepts += 1\n",
    "                        confidences_incorrect.append(conf)\n",
    "                    else:\n",
    "                        confidences_correct.append(conf)\n",
    "                else:  # Autorizzato\n",
    "                    total_auth += 1\n",
    "                    if final_pred == label:\n",
    "                        correct_auth += 1\n",
    "                        confidences_correct.append(conf)\n",
    "                    else:\n",
    "                        false_rejects += 1\n",
    "                        confidences_incorrect.append(conf)\n",
    "\n",
    "                if (label != -1 and final_pred == label) or (label == -1 and final_pred == -1):\n",
    "                    correct_total += 1\n",
    "\n",
    "    # Valori binari: 1 = autorizzato, 0 = non autorizzato\n",
    "    y_true = np.array([1 if l != -1 else 0 for l in all_labels])\n",
    "    y_pred = np.array([1 if p != -1 else 0 for p in all_preds])\n",
    "    y_scores = np.array(all_confs)\n",
    "\n",
    "    # Metriche di classificazione\n",
    "    overall_acc = 100 * correct_total / (total_auth + total_unauth) if (total_auth + total_unauth) > 0 else 0\n",
    "    auth_acc = 100 * correct_auth / total_auth if total_auth > 0 else 0\n",
    "    far = 100 * false_accepts / total_unauth if total_unauth > 0 else 0\n",
    "    frr = 100 * false_rejects / total_auth if total_auth > 0 else 0\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    avg_conf_correct = np.mean(confidences_correct) if confidences_correct else 0\n",
    "    avg_conf_incorrect = np.mean(confidences_incorrect) if confidences_incorrect else 0\n",
    "\n",
    "    # ROC & PR Curve\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores)\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "    if print_summary:\n",
    "        print(f\"\\n--- Evaluation Results ---\")\n",
    "        print(f\"Threshold: {threshold}\")\n",
    "        print(f\"Authorized Accuracy: {auth_acc:.2f}%\")\n",
    "        print(f\"False Accept Rate (FAR): {far:.2f}%\")\n",
    "        print(f\"False Reject Rate (FRR): {frr:.2f}%\")\n",
    "        print(f\"Overall Accuracy: {overall_acc:.2f}%\")\n",
    "        print(f\"Precision: {precision:.2f}%\")\n",
    "        print(f\"Recall (TPR): {recall:.2f}%\")\n",
    "        print(f\"F1 Score: {f1:.2f}%\")\n",
    "        print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "        print(f\"PR AUC: {pr_auc:.2f}\")\n",
    "        print(f\"Avg Confidence (Correct): {avg_conf_correct:.2f}\")\n",
    "        print(f\"Avg Confidence (Incorrect): {avg_conf_incorrect:.2f}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Output strutturato per eventuali plot successivi\n",
    "    return {\n",
    "        'authorized_accuracy': auth_acc,\n",
    "        'far': far,\n",
    "        'frr': frr,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'avg_conf_correct': avg_conf_correct,\n",
    "        'avg_conf_incorrect': avg_conf_incorrect,\n",
    "        'all_confs': all_confs,\n",
    "        'all_labels': all_labels,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'fpr': fpr.tolist(),\n",
    "        'tpr': tpr.tolist(),\n",
    "        'roc_thresholds': roc_thresholds.tolist(),\n",
    "        'precision_curve': precision_curve.tolist(),\n",
    "        'recall_curve': recall_curve.tolist(),\n",
    "        'pr_thresholds': pr_thresholds.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_metrics(metrics):\n",
    "    print(\"\\nEVALUATION SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'Authorized Accuracy':25}: {metrics['authorized_accuracy']:.2f}%\")\n",
    "    print(f\"{'False Accept Rate (FAR)':25}: {metrics['far']:.2f}%\")\n",
    "    print(f\"{'False Reject Rate (FRR)':25}: {metrics['frr']:.2f}%\")\n",
    "    print(f\"{'Overall Accuracy':25}: {metrics['overall_accuracy']:.2f}%\")\n",
    "    print()\n",
    "    print(f\"{'Precision':25}: {metrics['precision']:.2f}%\")\n",
    "    print(f\"{'Recall':25}: {metrics['recall']:.2f}%\")\n",
    "    print(f\"{'F1 Score':25}: {metrics['f1']:.2f}%\")\n",
    "    print()\n",
    "    print(f\"{'Avg Confidence (Correct)':25}: {metrics['avg_conf_correct']:.3f}\")\n",
    "    print(f\"{'Avg Confidence (Incorrect)':25}: {metrics['avg_conf_incorrect']:.3f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, labels=['Non-Auth', 'Auth']):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Confusion Matrix (Binary: Auth vs Non-Auth)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_bars(metrics):\n",
    "    names = ['Authorized Acc.', 'FAR', 'FRR', 'Overall Acc.', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = [\n",
    "        metrics['authorized_accuracy'],\n",
    "        metrics['far'],\n",
    "        metrics['frr'],\n",
    "        metrics['overall_accuracy'],\n",
    "        metrics['precision'],\n",
    "        metrics['recall'],\n",
    "        metrics['f1']\n",
    "    ]\n",
    "\n",
    "    colors = ['green', 'red', 'red', 'blue', 'orange', 'orange', 'purple']\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(names, values, color=colors)\n",
    "    plt.title(\"Key Evaluation Metrics\")\n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_curves(metrics, title_prefix=\"Validation\"):\n",
    "    # Controllo che i dati necessari siano presenti\n",
    "    if not all(key in metrics for key in ['fpr', 'tpr', 'precision_curve', 'recall_curve', 'roc_auc', 'pr_auc']):\n",
    "        print(\"Metriche ROC/PR incomplete, impossibile plottare.\")\n",
    "        return\n",
    "\n",
    "    fpr = metrics['fpr']\n",
    "    tpr = metrics['tpr']\n",
    "    precision_curve = metrics['precision_curve']\n",
    "    recall_curve = metrics['recall_curve']\n",
    "    roc_auc = metrics['roc_auc']\n",
    "    pr_auc = metrics['pr_auc']\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # ROC Curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\", color='blue')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"Random\")\n",
    "    plt.xlabel(\"False Positive Rate (FAR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "    plt.title(f\"{title_prefix} ROC Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall_curve, precision_curve, label=f\"AUC = {pr_auc:.2f}\", color='green')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"{title_prefix} Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_evaluation_report(metrics):\n",
    "    print_evaluation_metrics(metrics)\n",
    "    plot_evaluation_curves(metrics)\n",
    "    plot_confusion_matrix(metrics['confusion_matrix'])\n",
    "    plot_metric_bars(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_far_frr_vs_threshold_from_scores(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Plot FAR and FRR in funzione della soglia.\n",
    "    y_true: np.array, 1 = autorizzato, 0 = non autorizzato\n",
    "    y_scores: np.array, confidenza del modello (es. softmax max probability)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    far = fpr * 100\n",
    "    frr = (1 - tpr) * 100\n",
    "\n",
    "    # Equal Error Rate (dove FAR ≈ FRR)\n",
    "    eer_idx = np.nanargmin(np.abs(far - frr))\n",
    "    eer_threshold = thresholds[eer_idx]\n",
    "    eer = (far[eer_idx] + frr[eer_idx]) / 2\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(thresholds, far, label=\"FAR (False Accept Rate)\", color='red')\n",
    "    plt.plot(thresholds, frr, label=\"FRR (False Reject Rate)\", color='blue')\n",
    "    plt.axvline(x=eer_threshold, linestyle='--', color='gray', label=f\"EER Threshold = {eer_threshold:.2f}\")\n",
    "    plt.axhline(y=eer, linestyle='--', color='green', label=f\"EER = {eer:.2f}%\")\n",
    "\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Error Rate (%)\")\n",
    "    plt.title(\"FAR and FRR vs Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Retrieve metrics given a treshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = evaluate_base_model(model, val_loader, threshold=0.87, device=device)\n",
    "full_evaluation_report(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1 if l != -1 else 0 for l in metrics['all_labels']]\n",
    "y_scores = metrics['all_confs']\n",
    "plot_far_frr_vs_threshold_from_scores(np.array(y_true), np.array(y_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate ArcFace Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_class_centroids_from_loader(model, loader, device='cuda'):\n",
    "    \"\"\"Costruisce i centroidi per ciascuna classe *autorizzata* (label >= 0)\n",
    "    usando EMBEDDING L2-normalizzati.\n",
    "    Ritorna:\n",
    "      - centroids: Tensor [C, D]\n",
    "      - class_ids: lista ID classe (stesso ordine dei centroidi)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sums, counts = {}, {}\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            out = model(images)\n",
    "            embeddings = out[1] if (isinstance(out, tuple) and len(out) >= 2) else out\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "            for e, l in zip(embeddings, labels):\n",
    "                l = int(l.item())\n",
    "                if l < 0:      # ignora non autorizzati\n",
    "                    continue\n",
    "                if l not in sums:\n",
    "                    sums[l] = e.detach().clone()\n",
    "                    counts[l] = 1\n",
    "                else:\n",
    "                    sums[l] += e.detach()\n",
    "                    counts[l] += 1\n",
    "    if not sums:\n",
    "        raise RuntimeError(\"Nessuna classe autorizzata trovata per i centroidi.\")\n",
    "    class_ids = sorted(sums.keys())\n",
    "    centroids = []\n",
    "    for cid in class_ids:\n",
    "        c = sums[cid] / counts[cid]\n",
    "        centroids.append(F.normalize(c, dim=0))\n",
    "    centroids = torch.stack(centroids, dim=0)  # [C, D]\n",
    "    return centroids, class_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "\n",
    "def evaluate_openset_centroids(\n",
    "    model,\n",
    "    val_loader,\n",
    "    threshold=0.6,\n",
    "    device='cpu',\n",
    "    train_loader=None,\n",
    "    centroids=None,\n",
    "    class_ids=None,\n",
    "    print_summary=True,\n",
    "    return_raw=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Valutazione open-set basata su cosine vs CENTROIDI.\n",
    "    - Se 'centroids' e 'class_ids' non sono passati, vengono costruiti da 'train_loader'.\n",
    "    - Decisione: s_max = max_j cos( e , c_j ); se s_max < threshold => pred = -1 (impostore), altrimenti pred = class_ids[argmax].\n",
    "    - Metriche: FAR/FRR/accuracy + ROC/PR per *impostor detection* (y_true: 1=genuino, 0=impostore) usando s_max.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # --- centroidi ---\n",
    "    if centroids is None or class_ids is None:\n",
    "        if train_loader is None:\n",
    "            raise ValueError(\"Passa train_loader oppure centroids+class_ids.\")\n",
    "        centroids, class_ids = build_class_centroids_from_loader(model, train_loader, device=device)\n",
    "    centroids = F.normalize(centroids.to(device), p=2, dim=1)  # [C,D]\n",
    "\n",
    "    all_preds, all_labels, all_scores = [], [], []\n",
    "\n",
    "    total_auth = total_unauth = 0\n",
    "    correct_auth = correct_total = 0\n",
    "    false_accepts = false_rejects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            out = model(x)\n",
    "            emb = out[1] if (isinstance(out, tuple) and len(out) >= 2) else out\n",
    "            emb = F.normalize(emb, p=2, dim=1)               # [B,D]\n",
    "            scores = emb @ centroids.T                       # [B,C] cosine\n",
    "            s_max, j_max = scores.max(dim=1)                 # top-1\n",
    "            s_max = s_max.detach().cpu().numpy()\n",
    "            j_max = j_max.detach().cpu().numpy()\n",
    "            y_np  = y.detach().cpu().numpy()\n",
    "\n",
    "            for sm, jm, yy in zip(s_max, j_max, y_np):\n",
    "                pred = class_ids[int(jm)] if sm >= threshold else -1\n",
    "                all_preds.append(pred)\n",
    "                all_labels.append(int(yy))\n",
    "                all_scores.append(float(sm))\n",
    "\n",
    "                if yy == -1:          # impostore\n",
    "                    total_unauth += 1\n",
    "                    if pred != -1:    # false accept\n",
    "                        false_accepts += 1\n",
    "                    else:\n",
    "                        correct_total += 1\n",
    "                else:                  # genuino\n",
    "                    total_auth += 1\n",
    "                    if pred == yy:    # corretto\n",
    "                        correct_auth += 1\n",
    "                        correct_total += 1\n",
    "                    else:\n",
    "                        false_rejects += 1\n",
    "\n",
    "    # --- metriche aggregate ---\n",
    "    overall_acc = 100 * correct_total / (total_auth + total_unauth) if (total_auth + total_unauth) else 0.0\n",
    "    auth_acc    = 100 * correct_auth  / total_auth if total_auth else 0.0\n",
    "    far         = 100 * false_accepts / total_unauth if total_unauth else 0.0\n",
    "    frr         = 100 * false_rejects / total_auth if total_auth else 0.0\n",
    "\n",
    "    # impostor detection (binary): 1=genuino, 0=impostore\n",
    "    y_true  = np.array([1 if l != -1 else 0 for l in all_labels], dtype=int)\n",
    "    y_predb = np.array([1 if p != -1 else 0 for p in all_preds], dtype=int)\n",
    "    y_scores= np.array(all_scores, dtype=float)\n",
    "\n",
    "    precision = precision_score(y_true, y_predb, zero_division=0) * 100\n",
    "    recall    = recall_score(y_true, y_predb, zero_division=0) * 100\n",
    "    f1        = f1_score(y_true, y_predb, zero_division=0) * 100\n",
    "    cm        = confusion_matrix(y_true, y_predb)\n",
    "\n",
    "    # ROC / PR (sui punteggi s_max)\n",
    "    try:\n",
    "        roc_auc = float(roc_auc_score(y_true, y_scores))\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores)\n",
    "    except Exception:\n",
    "        roc_auc = float('nan'); fpr=tpr=roc_thresholds=np.array([])\n",
    "\n",
    "    try:\n",
    "        precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_scores)\n",
    "        pr_auc = float(auc(recall_curve, precision_curve))\n",
    "    except Exception:\n",
    "        precision_curve=recall_curve=pr_thresholds=np.array([]); pr_auc=float('nan')\n",
    "\n",
    "    if print_summary:\n",
    "        print(\"\\n--- Open-set Evaluation (centroids, cosine) ---\")\n",
    "        print(f\"Threshold: {threshold:.3f}  | Centroidi: {len(class_ids)}\")\n",
    "        print(f\"Authorized Accuracy: {auth_acc:.2f}%\")\n",
    "        print(f\"False Accept Rate (FAR): {far:.2f}%\")\n",
    "        print(f\"False Reject Rate (FRR): {frr:.2f}%\")\n",
    "        print(f\"Overall Accuracy: {overall_acc:.2f}%\")\n",
    "        print(f\"Precision: {precision:.2f}%  Recall: {recall:.2f}%  F1: {f1:.2f}%\")\n",
    "        print(f\"ROC AUC: {roc_auc:.3f}   PR AUC: {pr_auc:.3f}\")\n",
    "        print(f\"Confusion Matrix (impostor/genuine):\\n{cm}\")\n",
    "\n",
    "    out = {\n",
    "        'authorized_accuracy': auth_acc,\n",
    "        'far': far,\n",
    "        'frr': frr,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'all_scores': all_scores,                # s_max (cosine)\n",
    "        'all_labels': all_labels,                # label originali (-1 impostori)\n",
    "        'all_preds': all_preds,                  # pred = class_id o -1\n",
    "        # alias compatibili con il tuo codice precedente:\n",
    "        'max_probs_list': all_scores,            # alias → ora sono cosine\n",
    "        'labels_list': [1 if l != -1 else 0 for l in all_labels],\n",
    "        'fpr': fpr.tolist() if len(fpr) else [],\n",
    "        'tpr': tpr.tolist() if len(tpr) else [],\n",
    "        'roc_thresholds': roc_thresholds.tolist() if len(roc_thresholds) else [],\n",
    "        'precision_curve': precision_curve.tolist() if len(precision_curve) else [],\n",
    "        'recall_curve': recall_curve.tolist() if len(recall_curve) else [],\n",
    "        'pr_thresholds': pr_thresholds.tolist() if len(pr_thresholds) else [],\n",
    "        'prototypes': 'centroids',\n",
    "        'class_ids': class_ids,\n",
    "    }\n",
    "    if return_raw:\n",
    "        out['y_true'] = y_true\n",
    "        out['y_pred'] = y_predb\n",
    "        out['y_scores'] = y_scores\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Centroidi dalla GALLERIA (train)\n",
    "centroids, class_ids = build_class_centroids_from_loader(model, train_loader, device=device)\n",
    "res = evaluate_openset_centroids(model, val_loader, 0.74, device, centroids=centroids, class_ids=class_ids) #(treshold = EER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --------- Utils ---------\n",
    "def as_centroid_matrix(centroids, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Accetta:\n",
    "      - dict {label/name -> vector}\n",
    "      - torch.Tensor [C,D]\n",
    "      - np.ndarray [C,D]\n",
    "    Ritorna: torch.FloatTensor [C,D] L2-normalizzato su 'device'\n",
    "    \"\"\"\n",
    "    if isinstance(centroids, dict):\n",
    "        # ordina per chiave per avere un ordine stabile\n",
    "        keys = list(centroids.keys())\n",
    "        mat = np.stack([np.asarray(centroids[k], dtype=np.float32) for k in keys], axis=0)\n",
    "        C = torch.from_numpy(mat)\n",
    "    elif isinstance(centroids, np.ndarray):\n",
    "        C = torch.from_numpy(centroids.astype(np.float32))\n",
    "    elif torch.is_tensor(centroids):\n",
    "        C = centroids.float()\n",
    "    else:\n",
    "        raise TypeError(\"centroids deve essere dict, np.ndarray o torch.Tensor\")\n",
    "\n",
    "    C = F.normalize(C, p=2, dim=1)  # [C,D]\n",
    "    return C.to(device)\n",
    "\n",
    "# --------- Score collection (batch) ---------\n",
    "def collect_scores_centroids(val_loader, model, centroids, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Calcola per ogni campione la max cosine similarity verso i centroidi.\n",
    "    Input:\n",
    "      - val_loader: batch -> (images, labels) con labels: -1 impostore, >=0 genuino\n",
    "      - model: restituisce (logits, embeddings) oppure direttamente embeddings\n",
    "      - centroids: dict o [C,D]\n",
    "    Output:\n",
    "      - scores: np.array [N] con s_max per sample\n",
    "      - labels: np.array [N] con etichette vere (-1 impostore, >=0 genuino)\n",
    "    \"\"\"\n",
    "    C = as_centroid_matrix(centroids, device=device)  # [C,D]\n",
    "    model.eval()\n",
    "    scores, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, y_true in tqdm(val_loader, desc=\"Eval (centroids)\"):\n",
    "            images = images.to(device)\n",
    "            out = model(images)\n",
    "            emb = out[1] if (isinstance(out, tuple) and len(out) >= 2) else out\n",
    "            emb = F.normalize(emb, p=2, dim=1)        # [B,D]\n",
    "            s = emb @ C.T                              # [B,C] cosine\n",
    "            s_max, _ = torch.max(s, dim=1)            # [B]\n",
    "            scores.append(s_max.detach().cpu().numpy())\n",
    "            labels.append(y_true.detach().cpu().numpy())\n",
    "    scores = np.concatenate(scores, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0).astype(int)\n",
    "    return scores, labels\n",
    "\n",
    "# --------- FAR/FRR computation + plot ---------\n",
    "def compute_far_frr(scores, labels, thresholds):\n",
    "    \"\"\"\n",
    "    labels: -1 = impostore, >=0 = genuino\n",
    "    Ritorna liste in percentuale.\n",
    "    \"\"\"\n",
    "    fars, frrs = [], []\n",
    "    total_auth = int(np.sum(labels != -1))\n",
    "    total_unauth = int(np.sum(labels == -1))\n",
    "    for t in thresholds:\n",
    "        # 1 = accettato come genuino, -1 = respinto\n",
    "        accepted = (scores >= t)\n",
    "        # FRR: genuini respinti\n",
    "        false_rejects = int(np.sum((labels != -1) & (~accepted)))\n",
    "        # FAR: impostori accettati\n",
    "        false_accepts = int(np.sum((labels == -1) & (accepted)))\n",
    "        frr = (false_rejects / total_auth) * 100 if total_auth else 0.0\n",
    "        far = (false_accepts / total_unauth) * 100 if total_unauth else 0.0\n",
    "        frrs.append(frr)\n",
    "        fars.append(far)\n",
    "    return fars, frrs\n",
    "\n",
    "def plot_far_frr(scores, labels, n_thresholds=200, title=\"FAR and FRR vs Threshold (Centroids)\"):\n",
    "    thresholds = np.linspace(-0.2, 1.0, n_thresholds)  # includo un po' di margine\n",
    "    fars, frrs = compute_far_frr(scores, labels, thresholds)\n",
    "\n",
    "    # EER\n",
    "    diffs = np.abs(np.array(fars) - np.array(frrs))\n",
    "    idx_eer = int(np.argmin(diffs))\n",
    "    eer_thr = thresholds[idx_eer]\n",
    "    eer_val = (fars[idx_eer] + frrs[idx_eer]) / 2.0\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(thresholds, fars, label=\"FAR (False Accept Rate)\", linewidth=2)\n",
    "    plt.plot(thresholds, frrs, label=\"FRR (False Reject Rate)\", linewidth=2)\n",
    "    plt.axvline(eer_thr, linestyle=\"--\", color=\"gray\", label=f\"EER t={eer_thr:.3f}\")\n",
    "    plt.axhline(eer_val, linestyle=\"--\", color=\"gray\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Error Rate [%]\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, ls='--', alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"[INFO] EER ≈ {eer_val:.2f}% at threshold {eer_thr:.3f}\")\n",
    "    return eer_thr, eer_val\n",
    "\n",
    "scores, labels = collect_scores_centroids(val_loader, model, centroids, device=device)\n",
    "eer_thr, eer_val = plot_far_frr(scores, labels, n_thresholds=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation_curves(res)\n",
    "plot_confusion_matrix(res['confusion_matrix'])\n",
    "plot_metric_bars(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_probe_scores_vs_centroids(model, loader, centroids, class_ids, device='cuda'):\n",
    "    \"\"\"Per ogni probe calcola:\n",
    "       - label (int)\n",
    "       - s_max = max_j cos(emb, centroid_j)\n",
    "       - rank_pos (1=top1) se genuino, altrimenti None\n",
    "       - s_target = score contro la classe corretta (se genuino), altrimenti None\n",
    "    Ritorna: lista di dict.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            out = model(images)\n",
    "            embeddings = out[1] if (isinstance(out, tuple) and len(out) >= 2) else out\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "            scores = embeddings @ centroids.T  # [B, C]\n",
    "            for i in range(scores.size(0)):\n",
    "                lbl = int(labels[i].item())\n",
    "                s = scores[i]\n",
    "                s_max = float(torch.max(s).item())\n",
    "                rank_pos, s_target = None, None\n",
    "                if lbl >= 0:\n",
    "                    try:\n",
    "                        j = class_ids.index(lbl)\n",
    "                        s_target = float(s[j].item())\n",
    "                        rank_pos = 1 + int(torch.sum(s > s[j]).item())\n",
    "                    except ValueError:\n",
    "                        pass  # label non in galleria → trattalo come impostore\n",
    "                results.append({'label': lbl, 's_max': s_max, 'rank_pos': rank_pos, 's_target': s_target})\n",
    "    return results\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_probe_stats_with_preds(model, loader, centroids, class_ids, device='cuda'):\n",
    "    \"\"\"Ritorna, per ogni probe:\n",
    "       - label (int, -1 se impostore)\n",
    "       - s_max (float), pred_idx (int in [0..C-1]), pred_class (class_ids[pred_idx])\n",
    "       - rank_pos (se genuino), s_target (score verso la classe corretta se genuino)\n",
    "       - (opz) orig_label_src: id sorgente dell'impostore, se il loader lo fornisce\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if len(batch) == 2:\n",
    "                images, labels = batch\n",
    "                orig_src = None\n",
    "            else:\n",
    "                # se il tuo loader restituisce anche l'ID sorgente originale, catturarlo qui\n",
    "                images, labels, orig_src = batch[0], batch[1], batch[2]\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            out = model(images)\n",
    "            emb = out[1] if (isinstance(out, tuple) and len(out) >= 2) else out\n",
    "            emb = F.normalize(emb, p=2, dim=1)\n",
    "\n",
    "            scores = emb @ centroids.T  # [B,C] cosine\n",
    "            pred_vals, pred_idx = torch.max(scores, dim=1)  # top-1\n",
    "            pred_vals = pred_vals.detach().cpu().numpy()\n",
    "            pred_idx = pred_idx.detach().cpu().numpy()\n",
    "\n",
    "            scores_np = scores.detach().cpu().numpy()\n",
    "            labels_np = labels.detach().cpu().numpy()\n",
    "            orig_np = None if orig_src is None else np.array(orig_src)\n",
    "\n",
    "            for i in range(scores_np.shape[0]):\n",
    "                lbl = int(labels_np[i])\n",
    "                pidx = int(pred_idx[i])\n",
    "                smax = float(pred_vals[i])\n",
    "                rpos, star = None, None\n",
    "                if lbl >= 0:\n",
    "                    try:\n",
    "                        j = class_ids.index(lbl)\n",
    "                        star = float(scores_np[i, j])\n",
    "                        rpos = 1 + int(np.sum(scores_np[i, :] > scores_np[i, j]))\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                rec = {\n",
    "                    'label': lbl,\n",
    "                    's_max': smax,\n",
    "                    'pred_idx': pidx,\n",
    "                    'pred_class': int(class_ids[pidx]),\n",
    "                    'rank_pos': rpos,\n",
    "                    's_target': star\n",
    "                }\n",
    "                if orig_np is not None:\n",
    "                    rec['orig_impostor_id'] = int(orig_np[i])\n",
    "                results.append(rec)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def doddington_zoo_metrics(results, class_ids, threshold):\n",
    "    \"\"\"\n",
    "    Calcola:\n",
    "      - goat_rate[c]: quota di probe genuini della classe c che FALLISCONO (FRR_i) a soglia t\n",
    "      - lamb_rate[c]: quota di probe impostori accettati come c (FPIR per identità) a soglia t\n",
    "      - wolf_rate[src] (opz.): quota di probe dell'impostore 'src' che superano la soglia impersonando qualcuno\n",
    "                               (richiede 'orig_impostor_id' nei results)\n",
    "    Ritorna dict con 'goat_rate', 'lamb_rate', 'wolf_rate'(o None), e conteggi di supporto.\n",
    "    \"\"\"\n",
    "    class_ids = list(class_ids)\n",
    "    C = len(class_ids)\n",
    "    idx_of = {c:i for i,c in enumerate(class_ids)}\n",
    "\n",
    "    # separazione genuini / impostori\n",
    "    labels = np.array([r['label'] for r in results], dtype=int)\n",
    "    is_g = labels >= 0\n",
    "    is_i = ~is_g\n",
    "\n",
    "    # --- GOAT: FRR_i(t) per classe genuina c ---\n",
    "    genuini_tot = defaultdict(int)\n",
    "    genuini_ok  = defaultdict(int)  # accettati correttamente (rank1=c e s_max>=t)\n",
    "\n",
    "    for r in results:\n",
    "        if r['label'] >= 0:\n",
    "            c = r['label']\n",
    "            genuini_tot[c] += 1\n",
    "            if (r['s_max'] >= threshold) and (r['rank_pos'] == 1) and (r['pred_class'] == c):\n",
    "                genuini_ok[c] += 1\n",
    "\n",
    "    goat_rate = {}\n",
    "    for c in class_ids:\n",
    "        tot = genuini_tot.get(c, 0)\n",
    "        ok  = genuini_ok.get(c, 0)\n",
    "        goat_rate[c] = float(1.0 - (ok / tot)) if tot > 0 else np.nan  # FRR_i(t)\n",
    "\n",
    "    # --- LAMB: per ogni classe target c, quante volte un impostore viene accettato come c ---\n",
    "    impostori_tot = int(np.sum(is_i))\n",
    "    impostori_acc_come_c = defaultdict(int)\n",
    "\n",
    "    for r in results:\n",
    "        if r['label'] < 0:\n",
    "            if r['s_max'] >= threshold:\n",
    "                impostori_acc_come_c[r['pred_class']] += 1\n",
    "\n",
    "    lamb_rate = {}\n",
    "    for c in class_ids:\n",
    "        lamb_rate[c] = float(impostori_acc_come_c.get(c, 0) / impostori_tot) if impostori_tot > 0 else np.nan\n",
    "\n",
    "    # --- WOLF (opzionale): richiede 'orig_impostor_id' nei results ---\n",
    "    have_src = any(('orig_impostor_id' in r) for r in results if r['label'] < 0)\n",
    "    wolf_rate = None\n",
    "    if have_src:\n",
    "        att_tot = defaultdict(int)\n",
    "        att_succ = defaultdict(int)\n",
    "        for r in results:\n",
    "            if r['label'] < 0 and ('orig_impostor_id' in r):\n",
    "                src = r['orig_impostor_id']\n",
    "                att_tot[src] += 1\n",
    "                if r['s_max'] >= threshold:\n",
    "                    att_succ[src] += 1\n",
    "        wolf_rate = {src: (att_succ.get(src,0)/att_tot[src]) for src in att_tot if att_tot[src] > 0}\n",
    "\n",
    "    return {\n",
    "        'goat_rate': goat_rate,\n",
    "        'lamb_rate': lamb_rate,\n",
    "        'wolf_rate': wolf_rate,\n",
    "        'support': {\n",
    "            'genuine_counts': dict(genuini_tot),\n",
    "            'impostor_count': impostori_tot\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sweep_watchlist_metrics_multi_k(results, thresholds, ks=(1,5,10)):\n",
    "    \"\"\"\n",
    "    results: output di compute_probe_scores_vs_centroids (lista di dict)\n",
    "    thresholds: iterable di soglie\n",
    "    ks: tuple dei rank per cui calcolare DIR@k\n",
    "    Ritorna: dict con\n",
    "      - 't' (np.ndarray)\n",
    "      - 'FPIR' (np.ndarray)  = FAR(t) sugli impostori\n",
    "      - 'FRR'  (np.ndarray)  = 1 - DIR@1(t)\n",
    "      - 'GRR'  (np.ndarray)  = 1 - FPIR(t)\n",
    "      - 'DIR@k' (dict{k: np.ndarray})\n",
    "      - 'EER', 'EER_t', 'EER_idx'          (calcolati su DIR@1 vs FPIR)\n",
    "    \"\"\"\n",
    "    labels = np.array([r['label'] for r in results], dtype=int)\n",
    "    is_g = labels >= 0\n",
    "    is_i = ~is_g\n",
    "    n_g = int(is_g.sum())\n",
    "    n_i = int(is_i.sum())\n",
    "    if n_g == 0 or n_i == 0:\n",
    "        raise ValueError(\"Servono sia probe genuini sia impostori.\")\n",
    "\n",
    "    s_max = np.array([r['s_max'] for r in results], dtype=float)\n",
    "    # rank_pos: NaN per non definiti (impostori o genuini non mappabili)\n",
    "    rank_pos = np.array([ (r['rank_pos'] if r['rank_pos'] is not None else np.nan) for r in results ], dtype=float)\n",
    "\n",
    "    thresholds = np.array(list(thresholds), dtype=float)\n",
    "    FPIR = np.empty_like(thresholds)\n",
    "    DIRk = {k: np.empty_like(thresholds) for k in ks}\n",
    "\n",
    "    for idx, t in enumerate(thresholds):\n",
    "        # impostori accettati\n",
    "        FPIR[idx] = float((is_i & (s_max >= t)).sum()) / n_i\n",
    "        # genuini accettati e identificati entro k\n",
    "        mask_g = is_g & (s_max >= t)\n",
    "        for k in ks:\n",
    "            DIRk[k][idx] = float((mask_g & (rank_pos <= k)).sum()) / n_g\n",
    "\n",
    "    DIR1 = DIRk[min(ks)] if (1 in ks) else np.array([np.nan]*len(thresholds))\n",
    "    FRR  = 1.0 - DIR1\n",
    "    GRR  = 1.0 - FPIR\n",
    "\n",
    "    # EER su (DIR@1, FPIR)\n",
    "    diff = np.abs(FPIR - FRR)\n",
    "    eer_idx = int(np.nanargmin(diff))\n",
    "    EER   = float(0.5*(FPIR[eer_idx] + FRR[eer_idx]))\n",
    "    EER_t = float(thresholds[eer_idx])\n",
    "\n",
    "    return {\n",
    "        't': thresholds, 'FPIR': FPIR, 'FRR': FRR, 'GRR': GRR,\n",
    "        'DIR@k': DIRk, 'EER': EER, 'EER_t': EER_t, 'EER_idx': eer_idx\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_watchlist_roc_multi(curves_multi, ks=(1,5,10)):\n",
    "    FPIR = curves_multi['FPIR'] * 100.0\n",
    "    plt.figure(figsize=(6.2,5.2))\n",
    "    for k in ks:\n",
    "        DIRk = curves_multi['DIR@k'][k] * 100.0\n",
    "        plt.plot(FPIR, DIRk, label=f'DIR@rank-{k}', linewidth=2)\n",
    "    # marker EER su k=1 (se presente)\n",
    "    if 1 in ks and 'EER_idx' in curves_multi:\n",
    "        i = curves_multi['EER_idx']\n",
    "        plt.scatter([FPIR[i]], [curves_multi['DIR@k'][1][i]*100.0], s=40, color='red', zorder=3)\n",
    "        plt.annotate(f\"EER≈{curves_multi['EER']*100:.2f}%\\nt={curves_multi['EER_t']:.3f}\",\n",
    "                     (FPIR[i], curves_multi['DIR@k'][1][i]*100.0),\n",
    "                     textcoords='offset points', xytext=(8,-10))\n",
    "    plt.xlabel('FAR / FPIR [%]')\n",
    "    plt.ylabel('DIR@rank-k [%]')\n",
    "    plt.title('Open-set (Watchlist) ROC – multi-rank')\n",
    "    plt.grid(True, ls='--', alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_score_distributions(results):\n",
    "    \"\"\"Distribuzioni dei punteggi come in slide:\n",
    "       - p(s|H1): per genuini, score verso la classe corretta (s_target)\n",
    "       - p(s|H0): per impostori, max score (s_max) contro la galleria\n",
    "    \"\"\"\n",
    "    s_target = np.array([r['s_target'] for r in results if (r['s_target'] is not None)], dtype=float)\n",
    "    s_impostor = np.array([r['s_max'] for r in results if r['label'] < 0], dtype=float)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    if len(s_target) > 0:\n",
    "        plt.hist(s_target, bins=50, alpha=0.6, label='p(s|H1) genuini', density=True)\n",
    "    if len(s_impostor) > 0:\n",
    "        plt.hist(s_impostor, bins=50, alpha=0.6, label='p(s|H0) impostori', density=True)\n",
    "    plt.xlabel('Score (cosine similarity)')\n",
    "    plt.ylabel('Densità')\n",
    "    plt.legend()\n",
    "    plt.title('Distribuzioni dei punteggi')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_operating_point(curves_multi, t_op=None):\n",
    "    if t_op is None:\n",
    "        t_op = curves_multi['EER_t']\n",
    "    t = curves_multi['t']\n",
    "    i = int(np.argmin(np.abs(t - t_op)))\n",
    "    vals = {\n",
    "        't': float(t[i]),\n",
    "        'DIR@1': float(curves_multi['DIR@k'][1][i]),\n",
    "        'FPIR' : float(curves_multi['FPIR'][i]),\n",
    "        'FRR'  : float(curves_multi['FRR'][i]),\n",
    "        'GRR'  : float(curves_multi['GRR'][i]),\n",
    "    }\n",
    "    print(f\"[t={vals['t']:.3f}] DIR@1={vals['DIR@1']*100:.2f}%  FPIR={vals['FPIR']*100:.2f}%  \"\n",
    "          f\"FRR={vals['FRR']*100:.2f}%  GRR={vals['GRR']*100:.2f}%\")\n",
    "    return vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_threshold_sweeps_openset(curves_multi):\n",
    "    t = curves_multi['t']\n",
    "    DIR1 = curves_multi['DIR@k'][1] * 100.0 if 1 in curves_multi['DIR@k'] else None\n",
    "    FPIR = curves_multi['FPIR'] * 100.0\n",
    "    FRR  = curves_multi['FRR']  * 100.0\n",
    "    GRR  = curves_multi['GRR']  * 100.0\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    if DIR1 is not None:\n",
    "        plt.plot(t, DIR1, label='DIR@rank-1 [%]', linewidth=2)\n",
    "    plt.plot(t, FPIR, label='FPIR [%]', linewidth=2)\n",
    "    plt.plot(t, FRR,  label='FRR [%]',  linewidth=2)\n",
    "    plt.plot(t, GRR,  label='GRR [%]',  linewidth=2)\n",
    "    # punto EER\n",
    "    i = curves_multi['EER_idx']\n",
    "    plt.axvline(x=t[i], color='k', ls=':', alpha=0.6)\n",
    "    plt.annotate(f\"EER t={curves_multi['EER_t']:.3f}\",\n",
    "                 (t[i], max(FPIR[i], FRR[i])),\n",
    "                 textcoords='offset points', xytext=(8,8))\n",
    "    plt.xlabel('Soglia t')\n",
    "    plt.ylabel('Percentuale [%]')\n",
    "    plt.title('Sweep vs soglia: DIR@1, FPIR, FRR, GRR')\n",
    "    plt.grid(True, ls='--', alpha=0.35)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_goat_lamb(goat_rate, lamb_rate, top=30):\n",
    "    # ordina\n",
    "    goats = sorted([(c,v) for c,v in goat_rate.items() if not np.isnan(v)], key=lambda x: x[1], reverse=True)\n",
    "    lambs = sorted([(c,v) for c,v in lamb_rate.items() if not np.isnan(v)], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # GOAT\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sel = goats[:top]\n",
    "    plt.bar([str(c) for c,_ in sel], [v for _,v in sel])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('FRR_i(t) per classe')\n",
    "    plt.title(f'Doddington – GOATS (peggiori {min(top,len(sel))})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # LAMB\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sel = lambs[:top]\n",
    "    plt.bar([str(c) for c,_ in sel], [v for _,v in sel])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('FPIR verso classe (quota impostori accettati come c)')\n",
    "    plt.title(f'Doddington – LAMBS (peggiori {min(top,len(sel))})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def scatter_goat_vs_lamb(goat_rate, lamb_rate):\n",
    "    # scatter FRR_i vs FPIR_i per classe\n",
    "    keys = sorted(set(k for k in goat_rate.keys() if k in lamb_rate))\n",
    "    x = np.array([goat_rate[k] for k in keys])\n",
    "    y = np.array([lamb_rate[k] for k in keys])\n",
    "    mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x, y, keys = x[mask], y[mask], [keys[i] for i,m in enumerate(mask) if m]\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(x, y, s=20)\n",
    "    plt.xlabel('GOAT rate = FRR_i(t)')\n",
    "    plt.ylabel('LAMB rate = FPIR→i(t)')\n",
    "    plt.title('Doddington – mappa GOAT vs LAMB (per classe)')\n",
    "    plt.grid(True, ls='--', alpha=0.4)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Punteggi dei PROBE (val) contro i centroidi\n",
    "results = compute_probe_scores_vs_centroids(model, val_loader, centroids.to(device), class_ids, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Sweep soglia → metriche (k=1 misura principale)\n",
    "thresholds = np.linspace(-0.2, 1.0, 600)  # adatta se necessario\n",
    "curves_multi = sweep_watchlist_metrics_multi_k(results, thresholds, ks=(1,5,10))\n",
    "\n",
    "plot_watchlist_roc_multi(curves_multi, ks=(1,5,10))\n",
    "plot_threshold_sweeps_openset(curves_multi)\n",
    "print_operating_point(curves_multi)          # usa t_EER\n",
    "print_operating_point(curves_multi, t_op=0.792)  # oppure a soglia fissa\n",
    "\n",
    "\n",
    "plot_score_distributions(results)   # p(s|H1) vs p(s|H0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Statistiche probe→centroidi sul validation (open-set)\n",
    "stats = compute_probe_stats_with_preds(model, val_loader, centroids.to(device), class_ids, device=device)\n",
    "\n",
    "# 3) Soglia operativa: usa quella dell’EER che già calcoli (qui esempio t=curves['EER_t'])\n",
    "#    Se non l'hai ancora calcolata, fai lo sweep come già implementato e prendi la EER_t.\n",
    "t_oper = 0.792  # <-- SOSTITUISCI con la tua soglia (es. EER_t)\n",
    "\n",
    "# 4) Doddington’s Zoo\n",
    "zoo = doddington_zoo_metrics(stats, class_ids, threshold=t_oper)\n",
    "goat_rate = zoo['goat_rate']   # dict {class_id -> FRR_i(t)}\n",
    "lamb_rate = zoo['lamb_rate']   # dict {class_id -> FPIR→i(t)}\n",
    "wolf_rate = zoo['wolf_rate']   # dict opzionale {impostor_src -> successo}, o None\n",
    "\n",
    "# 5) Plot\n",
    "plot_goat_lamb(goat_rate, lamb_rate, top=30)\n",
    "scatter_goat_vs_lamb(goat_rate, lamb_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity distribution with centroids (genuines vs impostors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# -------------------- CONFIG (allinea a demo) --------------------\n",
    "DATA_ROOT      = '../../IAM+RIMES'          # root del dataset (ImageFolder)\n",
    "SPLIT_PATH     = 'splits/IAM+RIMES.pth'     # stesso split usato in app/test_set\n",
    "MODEL_PATH     = '../demo/arcface_full_model.pth'   # modello salvato intero (torch.save(model))\n",
    "OUT_NPY_NAMES  = 'author_centroids_names.npy'  # output: chiavi = NOME classe\n",
    "OUT_NPY_IDS    = 'author_centroids_ids.npy'    # output: chiavi = new_id (ricodificati)\n",
    "IMG_SIZE       = 128\n",
    "BATCH_SIZE     = 64\n",
    "NUM_WORKERS    = 0\n",
    "DEVICE         = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SEED           = 42\n",
    "LIMIT_BATCHES  = None   # es. 10 per test rapido; None per tutto il train\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -------------------- Utils --------------------\n",
    "def build_preprocess_from_model(model, img_size=128):\n",
    "    \"\"\"\n",
    "    Preprocess allineato al training:\n",
    "    - se conv1=1 -> grayscale 1 canale, Normalize((0.5,), (0.5,))\n",
    "    - se conv1=3 -> duplica il grigio su 3 canali, Normalize([0.5]*3, [0.5]*3)\n",
    "    (Niente mean/std ImageNet.)\n",
    "    \"\"\"\n",
    "    in_ch = 3\n",
    "    try:\n",
    "        in_ch = model.backbone.conv1.in_channels\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if in_ch == 1:\n",
    "        tf = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ])\n",
    "        mode = \"L\"\n",
    "    else:\n",
    "        tf = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "        mode = \"RGB\"\n",
    "    return tf, mode, in_ch\n",
    "\n",
    "class TransformedSubset(torch.utils.data.Dataset):\n",
    "    \"\"\"Applica transform e rimappa le label originali a new_id; -1 se non autorizzato.\"\"\"\n",
    "    def __init__(self, subset, transform, label_map):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map  # dict: original_idx -> new_id (solo autorizzati)\n",
    "    def __getitem__(self, idx):\n",
    "        img, orig_label = self.subset[idx]\n",
    "        new_label = self.label_map.get(orig_label, -1)\n",
    "        return self.transform(img), new_label   # Niente .convert('L'): lo decide il transform\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "def forward_to_embeddings(model: nn.Module, images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Estrae gli EMBEDDING evitando ArcFace/logits:\n",
    "    usa model.backbone -> model.embedding_layer -> L2 normalize.\n",
    "    Se questi attributi non esistono, cade su model(images) e prende embeddings dall'output.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, \"backbone\") and hasattr(model, \"embedding_layer\"):\n",
    "            feats = model.backbone(images)\n",
    "            emb = model.embedding_layer(feats)\n",
    "        else:\n",
    "            out = model(images)\n",
    "            if isinstance(out, tuple) and len(out) >= 2:\n",
    "                # supponiamo (logits, embeddings)\n",
    "                emb = out[1]\n",
    "            else:\n",
    "                emb = out\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb\n",
    "\n",
    "def l2_np(v, eps=1e-8):\n",
    "    v = np.asarray(v, dtype=np.float32)\n",
    "    n = np.linalg.norm(v) + eps\n",
    "    return v / n\n",
    "\n",
    "# -------------------- Main --------------------\n",
    "def main():\n",
    "    # Path assoluti per chiarezza\n",
    "    print(f\"[INFO] CWD: {os.getcwd()}\")\n",
    "    print(f\"[INFO] Split: {os.path.abspath(SPLIT_PATH)}\")\n",
    "    print(f\"[INFO] Modello: {os.path.abspath(MODEL_PATH)}\")\n",
    "\n",
    "    if not os.path.isfile(SPLIT_PATH):\n",
    "        raise FileNotFoundError(f\"Split non trovato: {SPLIT_PATH}\")\n",
    "    if not os.path.isfile(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Modello non trovato: {MODEL_PATH}\")\n",
    "\n",
    "    # Dataset base e split\n",
    "    dataset = ImageFolder(root=DATA_ROOT)  # senza transform\n",
    "    split = torch.load(SPLIT_PATH, map_location='cpu')\n",
    "    train_indices = split['train_indices']\n",
    "    label_map     = split['label_map']     # {original_idx -> new_id} solo autorizzati\n",
    "\n",
    "    # Mappature\n",
    "    authorized_orig_ids = set(label_map.keys())   # original_label_idx\n",
    "    new_to_orig = {new: orig for orig, new in label_map.items()}\n",
    "    idx_to_name = dataset.classes                 # original_label_idx -> nome cartella\n",
    "\n",
    "    # Subset train e DataLoader\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "\n",
    "    # Carica modello e preprocess coerente\n",
    "    model = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    if not isinstance(model, nn.Module):\n",
    "        raise TypeError(\"MODEL_PATH non contiene un torch.nn.Module (hai salvato solo lo state_dict?).\")\n",
    "    model.eval().to(DEVICE)\n",
    "\n",
    "    preprocess, color_mode, in_ch = build_preprocess_from_model(model, IMG_SIZE)\n",
    "    print(f\"[INFO] Device: {DEVICE} | conv1.in_channels={in_ch} | preprocess mode={color_mode}\")\n",
    "\n",
    "    train_data = TransformedSubset(train_subset, preprocess, label_map)\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=(DEVICE=='cuda'))\n",
    "\n",
    "    # Debug primo batch\n",
    "    for images, labels in train_loader:\n",
    "        print(f\"[DEBUG] Primo batch shape: {tuple(images.shape)} (atteso [B,{in_ch},{IMG_SIZE},{IMG_SIZE}])\")\n",
    "        break\n",
    "\n",
    "    # Contenitori centroidi\n",
    "    centroids = {}\n",
    "    counts = {}\n",
    "\n",
    "    # Progress (senza dipendenze esterne)\n",
    "    total_batches = (len(train_data) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    print(f\"[INFO] Batch totali: {total_batches} (BATCH_SIZE={BATCH_SIZE})\")\n",
    "\n",
    "    processed_batches = 0\n",
    "    try:\n",
    "        for bidx, (images, labels) in enumerate(train_loader, 1):\n",
    "            # Opzione per run veloce\n",
    "            if LIMIT_BATCHES is not None and bidx > LIMIT_BATCHES:\n",
    "                print(f\"[INFO] LIMIT_BATCHES={LIMIT_BATCHES} raggiunto, esco dal loop.\")\n",
    "                break\n",
    "\n",
    "            # Filtra solo autorizzati\n",
    "            mask = labels >= 0\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            images = images[mask].to(DEVICE, non_blocking=True)\n",
    "            labels = labels[mask].to(DEVICE, non_blocking=True)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings = forward_to_embeddings(model, images)  # [m, d]\n",
    "\n",
    "            # Accumulo per new_id\n",
    "            for e, l in zip(embeddings, labels):\n",
    "                l = int(l.item())  # new_id ricodificato\n",
    "                if l not in centroids:\n",
    "                    centroids[l] = e.detach().cpu()\n",
    "                    counts[l] = 1\n",
    "                else:\n",
    "                    centroids[l] += e.detach().cpu()\n",
    "                    counts[l] += 1\n",
    "\n",
    "            processed_batches += 1\n",
    "            if bidx % 20 == 0 or bidx == total_batches:\n",
    "                print(f\"[PROGRESS] batch {bidx}/{total_batches} \"\n",
    "                      f\"(centroidi parziali: {len(centroids)})\")\n",
    "\n",
    "        if not centroids:\n",
    "            raise RuntimeError(\"Nessun embedding raccolto: verifica che train_loader contenga autorizzati.\")\n",
    "\n",
    "        # Media + L2 finale\n",
    "        for l in list(centroids.keys()):\n",
    "            centroids[l] /= counts[l]\n",
    "            centroids[l] = F.normalize(centroids[l], dim=0)\n",
    "\n",
    "        # Diagnostica dimensione embedding\n",
    "        dims = {t.numel() for t in centroids.values()}\n",
    "        if len(dims) != 1:\n",
    "            raise RuntimeError(f\"Embedding dimension non uniforme nei centroidi: {dims}\")\n",
    "        embed_dim = dims.pop()\n",
    "        print(f\"[INFO] Centroidi calcolati: {len(centroids)} | dim={embed_dim}\")\n",
    "\n",
    "        # Mapping new_id -> NOME classe\n",
    "        author_centroids_names = {}\n",
    "        for new_id, vec in centroids.items():\n",
    "            if new_id not in new_to_orig:\n",
    "                continue\n",
    "            orig_idx = new_to_orig[new_id]\n",
    "            if orig_idx not in authorized_orig_ids:\n",
    "                continue\n",
    "            name = idx_to_name[orig_idx]\n",
    "            author_centroids_names[name] = vec.numpy()\n",
    "\n",
    "        if not author_centroids_names:\n",
    "            raise RuntimeError(\"Nessun centroide mappato a NOME: controlla label_map/new_to_orig e split.\")\n",
    "\n",
    "        # Opzionale: dizionario con chiavi = new_id\n",
    "        author_centroids_ids = {int(k): v.numpy() for k, v in centroids.items()}\n",
    "\n",
    "        # Salvataggio\n",
    "        np.save(OUT_NPY_NAMES, author_centroids_names)\n",
    "        np.save(OUT_NPY_IDS, author_centroids_ids)\n",
    "\n",
    "        print(f\"[OK] Salvati:\\n - {os.path.abspath(OUT_NPY_NAMES)} (chiavi = NOME)\\n\"\n",
    "              f\" - {os.path.abspath(OUT_NPY_IDS)} (chiavi = new_id)\")\n",
    "        print(f\"Esempio chiavi (names): {list(author_centroids_names.keys())[:10]}\")\n",
    "\n",
    "        # Copertura attesa\n",
    "        expected_auth = len(authorized_orig_ids)\n",
    "        got_names = len(author_centroids_names)\n",
    "        if got_names != expected_auth:\n",
    "            names_expected = set(idx_to_name[o] for o in authorized_orig_ids)\n",
    "            missing = sorted(list(names_expected - set(author_centroids_names.keys())))[:10]\n",
    "            print(f\"[WARN] Autorizzati nello split: {expected_auth} | Centroidi a nome: {got_names}\")\n",
    "            if missing:\n",
    "                print(f\"[INFO] Esempi mancanti (max 10): {missing}\")\n",
    "\n",
    "    finally:\n",
    "        # Salvataggio parziale (se qualcosa va storto)\n",
    "        if centroids:\n",
    "            tmp_path = OUT_NPY_IDS + \".partial.npy\"\n",
    "            np.save(tmp_path, {int(k): v.numpy() for k, v in centroids.items()})\n",
    "            print(f\"[SAFEGUARD] Parziale salvato: {os.path.abspath(tmp_path)}\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"arcface.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
